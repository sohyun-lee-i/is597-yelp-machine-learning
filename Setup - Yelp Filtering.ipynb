{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Filtering the Yelp Dataset",
   "id": "ae3d1bbd4d3aca43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This notebook contains steps for filtering the Yelp business dataset into a dataset containing only open restaurants in Idaho. The reviews for those restaurants are then added to the dataset.",
   "id": "4aa846d4abe0faf5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Download the files    ",
   "id": "7ee425e576ad049c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Download the dataset at https://www.yelp.com/dataset.",
   "id": "2b7ce0946a2feab0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Define input and output files.",
   "id": "2e0e66d037798dd5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The Yelp business and review json files are the inputs, and the output is a csv containing the filtered and merged data.",
   "id": "f1fa545bd5efd6e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T22:11:59.799215Z",
     "start_time": "2024-07-23T22:11:59.790736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "in_filename_businesses = 'yelp_academic_dataset_business.json'\n",
    "in_filename_reviews = 'yelp_academic_dataset_review.json'\n",
    "out_filename = 'yelp_filtered_dataset.csv'"
   ],
   "id": "dcaf3b5dccc026aa",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Filter the business dataset to only include open restaurants in Boise, Idaho.",
   "id": "e43938e63ccbb90d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For the remaining steps, I relied heavily on the instructions in https://towardsdatascience.com/converting-yelp-dataset-to-csv-using-pandas-2a4c8f03bd88.         ",
   "id": "61793daabb362ea5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import pandas as pd",
   "id": "df8818fc0c294977"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T22:13:49.311681Z",
     "start_time": "2024-07-23T22:13:43.674097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read the business file into a dataframe\n",
    "df_business = pd.read_json(in_filename_businesses, lines=True)\n",
    "\n",
    "# Filter the business dataframe to only include open restaurants in Boise\n",
    "df_restaurants = df_business[df_business['categories'].str.contains('Restaurants', case=False, na=False)]\n",
    "df_open_restaurants = df_restaurants[df_restaurants['is_open']==1]\n",
    "df_Boise_restaurants = df_open_restaurants[df_open_restaurants['city']=='Boise']\n",
    "\n",
    "# Drop unneeded columns\n",
    "columns_to_drop = ['address', 'city', 'state', 'latitude', 'longitude', 'is_open', 'hours']\n",
    "df_Boise_restaurants = df_Boise_restaurants.drop(columns_to_drop, axis=1)"
   ],
   "id": "57d971dbb88d93f8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Merge the relevant reviews into the dataset.",
   "id": "ffc2cb40067d61e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T22:16:29.558461Z",
     "start_time": "2024-07-23T22:16:29.547492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read the reviews file in chunks of 100,000. Set each column to the appropriate data type. Both of these steps reduce memory usage\n",
    "df_reviews = pd.read_json(in_filename_reviews, orient='records', lines=True,\n",
    "                       dtype={'review_id':str,'user_id':str,\n",
    "                              'business_id':str,'stars':int,\n",
    "                              'date':str,'text':str,'useful':int,\n",
    "                              'funny':int,'cool':int},\n",
    "                       chunksize=100000)"
   ],
   "id": "98dcc7fcc0599535",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T22:18:38.868099Z",
     "start_time": "2024-07-23T22:16:30.599100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read the review data and merge them to the business data in chunks\n",
    "chunk_list = []\n",
    "chunk_number = 0\n",
    "for chunk in df_reviews:\n",
    "     # Remove unneeded columns\n",
    "     chunk = chunk.drop(['user_id', 'review_id', 'useful','funny','cool'], axis=1)\n",
    "     # Rename the 'stars' column since the restaurants dataframe also has a 'stars' column\n",
    "     chunk = chunk.rename(columns={'stars': 'review_stars'})\n",
    "     # Inner merge with restaurants dataframe to include only the reviews for those restaurants\n",
    "     merged_chunk = pd.merge(df_Boise_restaurants, chunk, on='business_id', how='inner')\n",
    "     # Show feedback on progress\n",
    "     chunk_number += 1\n",
    "     print(f\"Working on chunk {chunk_number}...\")\n",
    "     chunk_list.append(merged_chunk)"
   ],
   "id": "760b3a11f0914974",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on chunk 1...\n",
      "Working on chunk 2...\n",
      "Working on chunk 3...\n",
      "Working on chunk 4...\n",
      "Working on chunk 5...\n",
      "Working on chunk 6...\n",
      "Working on chunk 7...\n",
      "Working on chunk 8...\n",
      "Working on chunk 9...\n",
      "Working on chunk 10...\n",
      "Working on chunk 11...\n",
      "Working on chunk 12...\n",
      "Working on chunk 13...\n",
      "Working on chunk 14...\n",
      "Working on chunk 15...\n",
      "Working on chunk 16...\n",
      "Working on chunk 17...\n",
      "Working on chunk 18...\n",
      "Working on chunk 19...\n",
      "Working on chunk 20...\n",
      "Working on chunk 21...\n",
      "Working on chunk 22...\n",
      "Working on chunk 23...\n",
      "Working on chunk 24...\n",
      "Working on chunk 25...\n",
      "Working on chunk 26...\n",
      "Working on chunk 27...\n",
      "Working on chunk 28...\n",
      "Working on chunk 29...\n",
      "Working on chunk 30...\n",
      "Working on chunk 31...\n",
      "Working on chunk 32...\n",
      "Working on chunk 33...\n",
      "Working on chunk 34...\n",
      "Working on chunk 35...\n",
      "Working on chunk 36...\n",
      "Working on chunk 37...\n",
      "Working on chunk 38...\n",
      "Working on chunk 39...\n",
      "Working on chunk 40...\n",
      "Working on chunk 41...\n",
      "Working on chunk 42...\n",
      "Working on chunk 43...\n",
      "Working on chunk 44...\n",
      "Working on chunk 45...\n",
      "Working on chunk 46...\n",
      "Working on chunk 47...\n",
      "Working on chunk 48...\n",
      "Working on chunk 49...\n",
      "Working on chunk 50...\n",
      "Working on chunk 51...\n",
      "Working on chunk 52...\n",
      "Working on chunk 53...\n",
      "Working on chunk 54...\n",
      "Working on chunk 55...\n",
      "Working on chunk 56...\n",
      "Working on chunk 57...\n",
      "Working on chunk 58...\n",
      "Working on chunk 59...\n",
      "Working on chunk 60...\n",
      "Working on chunk 61...\n",
      "Working on chunk 62...\n",
      "Working on chunk 63...\n",
      "Working on chunk 64...\n",
      "Working on chunk 65...\n",
      "Working on chunk 66...\n",
      "Working on chunk 67...\n",
      "Working on chunk 68...\n",
      "Working on chunk 69...\n",
      "Working on chunk 70...\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T22:24:30.762034Z",
     "start_time": "2024-07-23T22:24:30.724647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Concatenate the chunks into one dataframe\n",
    "df_yelp = pd.concat(chunk_list, ignore_index=True, join='outer', axis=0)"
   ],
   "id": "912e234143ef9e90",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Export the filtered and merged dataset to a CSV",
   "id": "9018d45db667fc8b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-07-23T22:24:36.212663Z"
    }
   },
   "cell_type": "code",
   "source": "df_yelp.to_csv(out_filename, index=False)",
   "id": "ef4f6918515055bc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
